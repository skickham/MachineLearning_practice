{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite\n",
    "\n",
    "```python \n",
    "pip install gensim\n",
    "```\n",
    "\n",
    "```bash\n",
    "pip install SQLAlchemy\n",
    "```\n",
    "```bash\n",
    "pip install Cython\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `CounterVectorizer` in scikit-learn package implements both tokenization and occurrence counting. The default configuration tokenizes the string by extracting words of at least 2 letters.\n",
    "- You can change it by setting the `token_pattern` parameter using regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'nlp': 4, u'love': 3, u'i': 2, u'over': 5, u'but': 0, u'python': 6, u'r': 8, u't': 9, u'don': 1, u'pythons': 7}\n",
      "[[0 0 1 1 0 1 1 0 1 0]\n",
      " [0 0 1 1 1 0 0 0 0 0]\n",
      " [1 1 1 1 0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1, token_pattern=r'\\b\\w+\\b')   # use this token to get words of at least one letter (default is two, ie. I and R would not be counted)\n",
    "# can be good ay of capturing words, but what about hyphens and contractions\n",
    "X = vectorizer.fit_transform([\"I love Python over R.\", \"I love NLP.\", \"But I don't love pythons\"])   # doesn't do any stemming\n",
    "print vectorizer.vocabulary_\n",
    "print X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, we are showing an example how to create a **bigram bag** of words matrix using the same `CountVectorizer` class from scikit learn.\n",
    "- The only difference is the `ngram_range` parameter. To get exact bigram, you use `ngram_range=(2,2)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'nlp': 13, u'love': 9, u'python': 20, u'over r': 19, u'over': 18, u'don t': 1, u'even r': 3, u'not': 14, u'i love': 6, u'r': 22, u'love other': 11, u'even': 2, u'don': 0, u'love python': 12, u'python over': 21, u'i': 4, u'love nlp': 10, u'not even': 15, u'languages': 7, u'other': 16, u't': 23, u'other languages': 17, u'i don': 5, u't love': 24, u'languages not': 8}\n",
      "[[0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0]\n",
      " [0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "Y = bigram_vectorizer.fit_transform([\"I love Python over R.\", \"I love NLP.\", \"I don't love other languages, not even R\"])\n",
    "print bigram_vectorizer.vocabulary_\n",
    "print Y.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily implement the N-gram function using pure Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('all', 'this'),\n",
       " ('this', 'happened'),\n",
       " ('happened', 'more'),\n",
       " ('more', 'or'),\n",
       " ('or', 'less')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list = ['all', 'this', 'happened', 'more', 'or', 'less']\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "    return zip(*[input_list[i:] for i in range(n)])   # what does the asterisk mean??? --> the asterisk indicates a nested list is going into the zip function\n",
    "find_ngrams(input_list, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['all', 'this', 'happened', 'more', 'or', 'less'],\n",
       " ['this', 'happened', 'more', 'or', 'less']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list = ['all', 'this', 'happened', 'more', 'or', 'less']\n",
    "\n",
    "[input_list[i:] for i in range(2)]    # so the zip-asterisk above knowcks off the final 'less' which has no partner "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of cooccurence matrix decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "la = np.linalg\n",
    "words = [\"I\", \"love\", \"Python\", \"over\", \"R\", \"NLP\", \".\"]\n",
    "X = np.array([[0,2,0,0,0,0,0],\n",
    "              [2,0,1,0,0,1,0],\n",
    "              [0,1,0,1,0,0,0],\n",
    "              [0,0,1,0,1,0,0],\n",
    "              [0,0,0,1,1,0,1],\n",
    "              [0,1,0,0,1,0,1],\n",
    "              [0,0,0,0,1,1,0]])\n",
    "U, s, Vh = la.svd(X, full_matrices=False)    # svd from linear algebra in numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa28ae48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "for i in range(len(words)):\n",
    "    plt.text(U[i,0], U[i,1], words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset\n",
    "\n",
    "- Today, we will use a database donated from Stefan Heinz, Yvonne Lau and Daniel Epstein. They scraped the reviews from [Metacritic](http://www.metacritic.com/), including games, tv shows and movies.\n",
    "- Check out their blog post [here](https://blog.nycdatascience.com/student-works/capstone/metarecommendr-recommendation-system-video-games-movies-tv-shows/) and also their awesome [Flask app](https://github.com/Steeefan/nycdsa-proj-04).\n",
    "- The following is the structure of the database:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Tables_in_capstone|\n",
    "| ------------- |\n",
    "|tblAvgRating   |\n",
    "| tblGame       |\n",
    "| tblMovie      |\n",
    "| tblReview     |\n",
    "| tblSysNice    |\n",
    "| tblTVShow     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "# Create a sql engine that connects to AWS RDS\n",
    "engine = create_engine('mysql://bootcamp:bcstudent@metacriticdata.ckfkocwbkmmc.us-west-2.rds.amazonaws.com:3306/capstone')\n",
    "# Load all the reviews\n",
    "reviews = pd.read_sql_query('SELECT * FROM tblReview;', engine)\n",
    "reviews = reviews.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load games dataset\n",
    "games = pd.read_sql_query('SELECT * FROM tblGame;', engine)\n",
    "games.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select the uniqueID and the name columns from games so we can merge it with reviews dataframe later.\n",
    "games = games[['uniqueID', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load tv shows dataset\n",
    "tv = pd.read_sql_query(\"SELECT * FROM tblTVShow;\", engine)\n",
    "tv = tv[['uniqueID', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load movies dataset\n",
    "movies = pd.read_sql_query(\"SELECT * FROM tblMovie;\", engine) \n",
    "movies = movies[['uniqueID', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concate three small tables to get a full list of item names\n",
    "ids = pd.concat([games, tv, movies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = pd.merge(reviews, ids, how='left', on='uniqueID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build word2vec model using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    " \n",
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "word2vec_model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keeping the input as a Python built-in list is convenient, but can use up a lot of RAM when the input is large.\n",
    "- Gensim only requires that the input must provide sentences sequentially, when iterated over. No need to keep everything in RAM: we can provide one sentence, process it, forget it, load another sentence…\n",
    "- The `simple_preprocess` function from gensim converts a document into a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "class MySentences(object): \n",
    "    def __iter__(self):\n",
    "        for i in range(reviews.shape[0]):\n",
    "            yield simple_preprocess(reviews.iloc[i,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `min-count` is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them. A reasonable value for min_count is between 0-100, depending on the size of your dataset.\n",
    "\n",
    "- `size` is the length of your output vector of each word, which is also the number of neurons in the hidden layer.\n",
    "\n",
    "- `workers` parameter has only effect if you have Cython installed. Without Cython, you’ll only be able to use one core because of the GIL (and word2vec training will be miserably slow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start traing word2vec model...\n",
      "CPU times: user 20min 35s, sys: 2min 6s, total: 22min 41s\n",
      "Wall time: 14min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import multiprocessing\n",
    "import os\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "if not os.path.exists('models/word2vec.model'):\n",
    "    print \"start traing word2vec model...\"\n",
    "    sentences = MySentences() # a memory-friendly iterator\n",
    "    word2vec_model = gensim.models.Word2Vec(sentences, min_count=20, size=200, workers=cores)\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "        word2vec_model.save('models/word2vec.model')\n",
    "    else:\n",
    "        word2vec_model.save('models/word2vec.model')\n",
    "else:\n",
    "    word2vec_model = Word2Vec.load('models/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model.wv.word_vec('movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also download some pre-trained model from [here](https://github.com/3Top/word2vec-api).\n",
    "- Most of them are trained using huge corpus, so the model would be around couple gigabytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a doc2vec model using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MyDocs reading from a data frame\n",
    "class MyDocs(object):\n",
    "    def __iter__(self):\n",
    "        for i in range(reviews.shape[0]):\n",
    "            yield TaggedDocument(words=simple_preprocess(reviews.iloc[i,5]), tags=['%s' % reviews.iloc[i,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start traing doc2vec model...\n",
      "CPU times: user 26min 40s, sys: 4min 42s, total: 31min 23s\n",
      "Wall time: 20min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists('models/doc2vec.model'):\n",
    "    print \"start traing doc2vec model...\"\n",
    "    documents = MyDocs()\n",
    "    doc2vec_model = Doc2Vec(dm=1, dbow_words=1, size=200, window=8, min_count=20, workers=cores)\n",
    "    doc2vec_model.build_vocab(documents)\n",
    "    doc2vec_model.train(documents, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.iter)\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "        doc2vec_model.save('models/doc2vec.model')\n",
    "    else:\n",
    "        doc2vec_model.save('models/doc2vec.model')\n",
    "else:\n",
    "    doc2vec_model = Doc2Vec.load('models/doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(keyword):\n",
    "    result = []\n",
    "    for name in reviews.name:\n",
    "        if keyword in name.lower():\n",
    "            result.append(name)\n",
    "    return set(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'La La Land', 'La La Land: Season 1'}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('la la land')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print doc2vec_model.docvecs.most_similar('La La Land', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
